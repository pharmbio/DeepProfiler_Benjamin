{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import sklearn.metrics\n",
    "\n",
    "import scipy.linalg\n",
    "import scipy.spatial.distance\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rc('ytick', labelsize=7)\n",
    "import seaborn as sns\n",
    "import os\n",
    "import psutil\n",
    "import polars as pl\n",
    "import cuml\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np \n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "featdir = \"outputs/results/\"\n",
    "PROJECT_ROOT = \"/share/data/analyses/benjamin/Single_cell_project/DP_BEACTICA/\"\n",
    "REG_PARAM = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_file_with_string(directory, string):\n",
    "    \"\"\"\n",
    "    Finds a file in the specified directory that contains the given string in its name.\n",
    "\n",
    "    Args:\n",
    "    directory (str): The directory to search in.\n",
    "    string (str): The string to look for in the file names.\n",
    "\n",
    "    Returns:\n",
    "    str: The path to the first file found that contains the string. None if no such file is found.\n",
    "    \"\"\"\n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"The directory {directory} does not exist.\")\n",
    "        return None\n",
    "\n",
    "    # Iterate through all files in the directory\n",
    "    for file in os.listdir(directory):\n",
    "        if string in file:\n",
    "            return os.path.join(directory, file)\n",
    "\n",
    "    # Return None if no file is found\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load metadata and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv(os.path.join(PROJECT_ROOT, \"inputs\", \"metadata\", \"metadata_deepprofiler_beactica.csv\")).drop_duplicates(inplace = False)\n",
    "meta = meta.sort_values(by=['Metadata_Well', 'Metadata_Site'])\n",
    "meta['Metadata_cmpdName'] = meta['Metadata_cmpdName'].str.upper()\n",
    "meta[\"Metadata_cmpdNameConc\"] = meta[\"Metadata_cmpdName\"] +   \" \" + meta[\"Metadata_cmpdConc\"].astype(str)\n",
    "meta_pl = pl.DataFrame(meta).drop('Unnamed: 0.1', 'Unnamed: 0', \"AR\", \"ER\", \"RNA\", \"AGP\", \"DNA\", \"Mito\")\n",
    "meta_pl = meta_pl.unique()\n",
    "\n",
    "validation = meta_pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/share/data/analyses/benjamin/Single_cell_project/DP_BEACTICA/outputs/results/features/PB000052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "expected a file path; '/share/data/analyses/benjamin/Single_cell_project/DP_BEACTICA/outputs/results/features/PB000052' is a directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(plates):\n\u001b[1;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(find_file_with_string(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(PROJECT_ROOT, featdir), p))\n\u001b[0;32m----> 7\u001b[0m     feature_df \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39;49mread_parquet(find_file_with_string(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(PROJECT_ROOT, featdir), p))\n\u001b[1;32m      8\u001b[0m     feature_df \u001b[39m=\u001b[39m feature_df\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m      9\u001b[0m     validation,\n\u001b[1;32m     10\u001b[0m     on\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mMetadata_Plate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMetadata_Well\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMetadata_Site\u001b[39m\u001b[39m'\u001b[39m],  \u001b[39m# columns to join on\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     how\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39minner\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m     master_df \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mconcat([master_df, feature_df])\n",
      "File \u001b[0;32m~/share/data/analyses/benjamin/Single_cell_project_rapids/rapids_new/lib/python3.10/site-packages/polars/io/parquet/functions.py:110\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(source, columns, n_rows, use_pyarrow, memory_map, storage_options, parallel, row_count_name, row_count_offset, low_memory, pyarrow_options, use_statistics, rechunk)\u001b[0m\n\u001b[1;32m    107\u001b[0m storage_options \u001b[39m=\u001b[39m storage_options \u001b[39mor\u001b[39;00m {}\n\u001b[1;32m    108\u001b[0m pyarrow_options \u001b[39m=\u001b[39m pyarrow_options \u001b[39mor\u001b[39;00m {}\n\u001b[0;32m--> 110\u001b[0m \u001b[39mwith\u001b[39;00m _prepare_file_arg(\n\u001b[1;32m    111\u001b[0m     source, use_pyarrow\u001b[39m=\u001b[39;49muse_pyarrow, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mstorage_options\n\u001b[1;32m    112\u001b[0m ) \u001b[39mas\u001b[39;00m source_prep:\n\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m use_pyarrow:\n\u001b[1;32m    114\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _PYARROW_AVAILABLE:\n",
      "File \u001b[0;32m~/share/data/analyses/benjamin/Single_cell_project_rapids/rapids_new/lib/python3.10/site-packages/polars/io/_utils.py:161\u001b[0m, in \u001b[0;36m_prepare_file_arg\u001b[0;34m(file, encoding, use_pyarrow, raise_if_empty, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m infer_storage_options(file)[\u001b[39m\"\u001b[39m\u001b[39mprotocol\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    158\u001b[0m     \u001b[39m# (lossy) utf8\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[39mif\u001b[39;00m has_utf8_utf8_lossy_encoding:\n\u001b[1;32m    160\u001b[0m         \u001b[39mreturn\u001b[39;00m managed_file(\n\u001b[0;32m--> 161\u001b[0m             normalize_filepath(file, check_not_directory\u001b[39m=\u001b[39;49mcheck_not_dir)\n\u001b[1;32m    162\u001b[0m         )\n\u001b[1;32m    163\u001b[0m     \u001b[39m# decode first\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[39mwith\u001b[39;00m Path(file)\u001b[39m.\u001b[39mopen(encoding\u001b[39m=\u001b[39mencoding_str) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/share/data/analyses/benjamin/Single_cell_project_rapids/rapids_new/lib/python3.10/site-packages/polars/utils/various.py:223\u001b[0m, in \u001b[0;36mnormalize_filepath\u001b[0;34m(path, check_not_directory)\u001b[0m\n\u001b[1;32m    217\u001b[0m path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexpanduser(path)  \u001b[39m# noqa: PTH111\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    219\u001b[0m     check_not_directory\n\u001b[1;32m    220\u001b[0m     \u001b[39mand\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(path)  \u001b[39m# noqa: PTH110\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[39mand\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(path)  \u001b[39m# noqa: PTH112\u001b[39;00m\n\u001b[1;32m    222\u001b[0m ):\n\u001b[0;32m--> 223\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIsADirectoryError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected a file path; \u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m!r}\u001b[39;00m\u001b[39m is a directory\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    224\u001b[0m \u001b[39mreturn\u001b[39;00m path\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: expected a file path; '/share/data/analyses/benjamin/Single_cell_project/DP_BEACTICA/outputs/results/features/PB000052' is a directory"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import polars as pl\n",
    "plates = validation[\"Metadata_Plate\"].unique()\n",
    "feat_out = \"outputs/results/parquets\"\n",
    "master_df = pl.DataFrame()\n",
    "for p in tqdm.tqdm(plates):\n",
    "    feature_df = pl.read_parquet(find_file_with_string(os.path.join(PROJECT_ROOT, featdir), p))\n",
    "    feature_df = feature_df.join(\n",
    "    validation,\n",
    "    on=['Metadata_Plate', 'Metadata_Well', 'Metadata_Site'],  # columns to join on\n",
    "    how='inner')\n",
    "    master_df = pl.concat([master_df, feature_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple MoA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = (meta_pl.groupby(['Site', 'Plate', 'Well', 'Metadata_cmpdName'])\n",
    "               .agg(pl.count())\n",
    "               .filter(pl.col('count') > 1))\n",
    "\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [col for col in master_df.columns if \"Feature\" in col]\n",
    "meta_features = [ col for col in master_df.columns if col not in features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import pycytominer as pm\n",
    "\n",
    "def prep_data(df1, features, meta_features, plate):\n",
    "    mask = ~df1['Metadata_cmpdName'].isin(['BLANK', 'UNTREATED', 'null'])\n",
    "    filtered_df = df1[mask]\n",
    "    temp1 = filtered_df.copy()\n",
    "    temp1 = temp1.loc[(temp1['Metadata_Plate'] == plate)]\n",
    "    data_norm = pm.normalize(profiles = temp1, features =  features, meta_features = meta_features, samples = \"Metadata_cmpdName == '[DMSO]'\", method = \"mad_robustize\")\n",
    "    print(\"Feature selection starts, shape:\", data_norm.shape)\n",
    "    df_selected = pm.feature_select(data_norm, features = features, operation = ['correlation_threshold', 'drop_na_columns'], corr_threshold=0.8)\n",
    "    print('Number of columns removed:', data_norm.shape[1] - df_selected.shape[1])\n",
    "    removed_cols = set(data_norm.columns) - set(df_selected.columns)\n",
    "    #out = df_selected.dropna().reset_index(drop = True)\n",
    "    #print('Number of NA rows removed:', df_selected.shape[0] - out.shape[0])\n",
    "    df_selected[\"Metadata_cmpdNameConc\"] = df_selected[\"Metadata_cmpdName\"] + df_selected[\"Metadata_cmpdConc\"].astype(str)\n",
    "    return data_norm, removed_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = list(master_df[\"Metadata_cmpdName\"].unique())\n",
    "mad_norm_df = pl.DataFrame()\n",
    "drop_cols = {}\n",
    "for p in tqdm.tqdm(plates):\n",
    "    filtered_df = master_df.filter(pl.col('Metadata_Plate') == p)\n",
    "    temp_pandas = filtered_df.to_pandas()\n",
    "    temp_processed, dropped_cols = prep_data(temp_pandas, features, meta_features, p)\n",
    "    drop_cols[p] = list(dropped_cols)\n",
    "    temp_polars = pl.from_pandas(temp_processed)\n",
    "    mad_norm_df = pl.concat([mad_norm_df, temp_polars])\n",
    "\n",
    "mad_norm_df = mad_norm_df.filter(pl.col(\"Metadata_cmpdName\").is_not_null())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = list(set().union(*drop_cols.values()))\n",
    "mad_norm_df = mad_norm_df.drop(cols_to_drop)\n",
    "features_fixed = [col for col in mad_norm_df.columns if col not in meta_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mad_norm_df.write_parquet('/home/jovyan/share/data/analyses/benjamin/Single_cell_project_rapids/sc_profiles_normalized_BEACTICA.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
